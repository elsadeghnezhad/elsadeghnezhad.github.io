<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Learn AI Basics | Elham Sadeghnezhad</title>
  <style>
    body {
      background-color: #0e0e2c;
      color: #ffffff;
      font-family: 'Fira Code', monospace;
      padding: 2rem;
    }
    h1 {
      color: #00ffd5;
    }
    h2 {
      color: #66e0ff;
      margin-top: 1.5rem;
    }
    p {
      margin-bottom: 1rem;
    }
    a {
      color: #00ffd5;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .roadmap {
      margin-top: 2rem;
      border-left: 3px solid #00ffd5;
      padding-left: 1rem;
    }
    .roadmap h3 {
      margin-top: 1rem;
      color: #00ffd5;
    }
    .roadmap ul {
      list-style-type: square;
      margin-left: 1.5rem;
    }
  </style>
</head>
<body>
  <h1> From Neurons to Attention</h1>
  <p>Welcome to your practical Journey into Deep Learning, Attention, and LLMs! This page covers essential concepts to help you understand how large language models, such as GPT, work.</p>

  <!-- <h1>üìò Learn the Basics of Large Language Models (LLMs)</h1>
  <p>Welcome to your beginner's guide to LLMs! This page covers essential concepts to help you understand how large language models like GPT work.</p> -->

  <!-- <h2>üî° What is an LLM?</h2>
  <p>Large Language Models (LLMs) are AI systems trained on vast amounts of text data. They can understand and generate human-like text based on context, grammar, and meaning.</p>

  <h2>üß± Key Concepts</h2>
  <ul>
    <li><strong>Tokenization:</strong> Breaking text into small pieces (tokens) the model can understand.</li>
    <li><strong>Transformers:</strong> The neural network architecture behind modern LLMs, allowing for attention-based learning.</li>
    <li><strong>Training:</strong> Feeding models large datasets and adjusting weights to predict the next token in a sequence.</li>
    <li><strong>Fine-Tuning:</strong> Adapting a pretrained model to a specific task or dataset.</li>
  </ul> -->

  <!-- <h2>üöÄ Tools You Can Use</h2>
  <ul>
    <li><a href="https://huggingface.co" target="_blank">Hugging Face</a> ‚Äì Open-source models and transformers</li>
    <li><a href="https://platform.openai.com" target="_blank">OpenAI API</a> ‚Äì Access GPT models via API</li>
    <li><a href="https://langchain.com" target="_blank">LangChain</a> ‚Äì Framework for LLM-powered applications</li>
  </ul> -->

  <div class="roadmap">

    <h3><a href="deep_learning_basics.html">üß† Deep Learning Basics</a></h3>
    <ul>
      <li>Neural Networks</li>
      <li>Deep Neural Networks</li>
      <li>Convolutional Neural Networks</li>
      <li>Recurrent Neural Networks and LSTMs</li>
      <li>All You Need Is Attention</li>
    </ul>

    <!-- <h2>üó∫Ô∏è LLM Learning Roadmap</h2>
    <h3><a href="llm_foundations.html">1. Foundations of AI</a></h3>  
    <ul>
      <li>Math for ML (Linear Algebra, Probability, Calculus)</li>
      <li>Python Programming</li>
      <li>Basic ML Concepts (Supervised, Unsupervised, Overfitting)</li>
    </ul>

    

    <h3><a href="nlp_fundamentals.html">3. Dive into NLP</a></h3>
    <ul>
      <li>Text Preprocessing (Tokenization, Lemmatization)</li>
      <li>Word Embeddings (Word2Vec, GloVe)</li>
      <li>Sequence Models (RNNs, LSTMs)</li>
    </ul>

    <h3><a href="transformers_attention.html">4. Transformers & Attention</a></h3>
    <ul>
      <li>Attention Mechanism</li>
      <li>Transformer Architecture (Vaswani et al.)</li>
      <li>Positional Encoding</li>
    </ul>

    <h3><a href="llm_architecture_training.html">5. Large Language Models</a></h3>
    <ul>
      <li>Pretraining vs Fine-Tuning</li>
      <li>GPT, BERT, T5 Overview</li>
      <li>Prompt Engineering</li>
    </ul>

    <h3>6. Hands-On Practice</h3>
    <ul>
      <li>Use Hugging Face Transformers</li>
      <li>Experiment with OpenAI & Cohere APIs</li>
      <li>Build LLM-powered apps using LangChain or Streamlit</li>
    </ul>

    <h3>7. Ethics & Safety</h3>
    <ul>
      <li>Bias in Language Models</li>
      <li>Explainability</li>
      <li>Responsible Deployment</li>
    </ul> -->
  </div>

  <p><a href="index.html">‚¨ÖÔ∏è Back to Portfolio</a></p>
</body>
</html>
