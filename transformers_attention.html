<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Transformers & Attention | LLM Roadmap</title>
  <style>
    body {
      background-color: #0e0e2c;
      color: #ffffff;
      font-family: 'Fira Code', monospace;
      padding: 2rem;
    }
    h1 {
      color: #00ffd5;
    }
    h2 {
      color: #66e0ff;
      margin-top: 1.5rem;
    }
    ul {
      margin-left: 1.5rem;
      list-style-type: square;
    }
    a {
      color: #00ffd5;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <h1>‚ö° Transformers & Attention</h1>

  <h2>üîé Attention Mechanism</h2>
  <ul>
    <li><strong>Self-Attention:</strong> Allows a model to weigh the importance of each word relative to others</li>
    <li><strong>Scaled Dot-Product:</strong> Computes attention weights</li>
    <li><strong>Multi-Head Attention:</strong> Captures different types of relationships in parallel</li>
  </ul>

  <h2>üß† Transformer Architecture</h2>
  <ul>
    <li>Encoder-Decoder structure (used in models like T5)</li>
    <li>Encoder-only models (e.g. BERT) for classification</li>
    <li>Decoder-only models (e.g. GPT) for generation</li>
  </ul>

  <h2>üìö Key Concepts</h2>
  <ul>
    <li>Positional Encoding: Adds sequence order information</li>
    <li>Layer Normalization & Residual Connections</li>
    <li>Stacking layers to increase model depth</li>
  </ul>

  <p><a href="ai-basics.html">‚¨ÖÔ∏è Back to Roadmap</a></p>
</body>
</html>
